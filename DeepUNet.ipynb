{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339},{"sourceId":1301322,"sourceType":"datasetVersion","datasetId":752995}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install segmentation_models_pytorch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport os\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset, Dataset, DataLoader, random_split\nfrom collections import Counter\nimport shutil\nfrom sklearn.cluster import KMeans\nimport cv2\nimport glob\nfrom tqdm import tqdm\nfrom timeit import default_timer as timer\nimport torchvision.models as models\nimport torch.optim as optim\nimport copy\nimport os\nimport random\nimport shutil\nimport zipfile\nfrom math import atan2, cos, sin, sqrt, pi, log\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom numpy import linalg as LA\nfrom torch import optim, nn\nfrom torch.utils.data.dataset import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T07:38:28.320465Z","iopub.execute_input":"2025-03-11T07:38:28.320816Z","iopub.status.idle":"2025-03-11T07:38:35.199917Z","shell.execute_reply.started":"2025-03-11T07:38:28.320789Z","shell.execute_reply":"2025-03-11T07:38:35.198894Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.4.0)\nRequirement already satisfied: efficientnet-pytorch>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.29.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\nRequirement already satisfied: pretrainedmodels>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\nRequirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels>=0.7.1->segmentation_models_pytorch) (4.0.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\n\ndef merge_folders(folder1, folder2, destination_folder):\n    # Create the destination folder if it doesn't exist\n    if not os.path.exists(destination_folder):\n        os.makedirs(destination_folder)\n\n    # Copy images from folder1 to destination_folder\n    for filename in os.listdir(folder1):\n        source_path = os.path.join(folder1, filename)\n        destination_path = os.path.join(destination_folder, filename)\n        shutil.copy2(source_path, destination_path)\n\n    # Copy images from folder2 to destination_folder\n    for filename in os.listdir(folder2):\n        source_path = os.path.join(folder2, filename)\n        destination_path = os.path.join(destination_folder, filename)\n        \n        # Handle duplicate filenames by renaming them\n        if os.path.exists(destination_path):\n            base, extension = os.path.splitext(filename)\n            new_filename = f\"{base}_copy{extension}\"\n            destination_path = os.path.join(destination_folder, new_filename)\n        \n        shutil.copy2(source_path, destination_path)\n\n# Replace with the paths of your source folders and destination folder\nfolder1 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1'\nfolder2 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2'\ndestination_folder = '/kaggle/working/train_image'\n\nmerge_folders(folder1, folder2, destination_folder)\n\nprint(\"Images merged successfully!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seeds(seed: int=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\nNUM_WORKERS = os.cpu_count()\n\nimage_size = (256, 256)\nheight, width = image_size\nbatch_size = 32\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T07:38:35.201194Z","iopub.execute_input":"2025-03-11T07:38:35.201652Z","iopub.status.idle":"2025-03-11T07:38:35.233603Z","shell.execute_reply.started":"2025-03-11T07:38:35.201624Z","shell.execute_reply":"2025-03-11T07:38:35.232865Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nclass SkinDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.image_names = [file for file in os.listdir(image_dir) if file.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        image_name = self.image_names[idx]\n        image_path = os.path.join(self.image_dir, image_name)\n        \n        # Construct the corresponding mask path\n        mask_name = image_name.replace('.jpg', '_segmentation.png')\n        mask_path = os.path.join(self.mask_dir, mask_name)\n\n        # Load the image and mask\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        # Ensure mask is binary (0 and 1) if needed\n        mask = (mask > 0).astype('float32')\n\n        # Apply transformations if provided\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask']\n\n        return image, mask\n\n# Define transformations using Albumentations\ntransform = A.Compose([\n    A.Resize(image_size[0], image_size[1]),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\nimage_dir = '/kaggle/working/train_image'\nmask_dir = '/kaggle/input/ham10000-lesion-segmentations/HAM10000_segmentations_lesion_tschandl'\n\n# Load dataset\ndataset = SkinDataset(image_dir, mask_dir, transform=transform)\n\n# Split dataset into training and testing\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T07:38:35.789781Z","iopub.execute_input":"2025-03-11T07:38:35.790079Z","iopub.status.idle":"2025-03-11T07:38:36.566984Z","shell.execute_reply.started":"2025-03-11T07:38:35.790056Z","shell.execute_reply":"2025-03-11T07:38:36.566053Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport segmentation_models_pytorch as smp\n\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n        super().__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, padding=padding, groups=in_channels)\n        self.bn = nn.BatchNorm2d(in_channels)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n        return self.relu(x)\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None, extra_concat_channels=0):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(mid_channels + extra_concat_channels, out_channels, kernel_size=3, padding=1)\n\n    def forward(self, x, extra_concat=None):\n        x = self.relu(self.conv1(x))\n        if extra_concat is not None:\n            x = torch.cat([x, extra_concat], dim=1)\n        x = self.relu(self.conv2(x))\n        return x\n\nclass UpSample(nn.Module):\n    def __init__(self, in_channels, out_channels, skip_in_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        self.depthwise_conv = DepthwiseSeparableConv(skip_in_channels, skip_in_channels)\n        self.attention = nn.Sequential(\n            nn.Conv2d(skip_in_channels, skip_in_channels // 8, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(skip_in_channels // 8, skip_in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.conv = DoubleConv(in_channels // 2 + skip_in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x2 = self.depthwise_conv(x2)\n        attn = self.attention(x2)\n        x2 = x2 * attn  # Apply attention to skip connection\n        x = torch.cat([x1, x2], dim=1)\n        return self.conv(x)\n\nclass ASPPBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.aspp1 = nn.Conv2d(in_channels, in_channels // 4, kernel_size=1)\n        self.aspp2 = nn.Conv2d(in_channels, in_channels // 4, kernel_size=3, padding=1, dilation=1)\n        self.aspp3 = nn.Conv2d(in_channels, in_channels // 4, kernel_size=3, padding=2, dilation=2)\n        self.aspp4 = nn.Conv2d(in_channels, in_channels // 4, kernel_size=3, padding=3, dilation=3)\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        aspp1 = self.aspp1(x)\n        aspp2 = self.aspp2(x)\n        aspp3 = self.aspp3(x)\n        aspp4 = self.aspp4(x)\n        x = torch.cat([aspp1, aspp2, aspp3, aspp4], dim=1)\n        return self.relu(self.conv(x))\n\nclass CustomUNetWithEfficientNetB3(nn.Module):\n    def __init__(self, num_classes=1, pretrained=True):\n        super().__init__()\n        self.encoder = smp.encoders.get_encoder(\n            \"efficientnet-b3\",\n            weights=\"imagenet\" if pretrained else None,\n        )\n        encoder_channels = self.encoder.out_channels\n\n        self.bottleneck = ASPPBottleneck(encoder_channels[-1], 1024)\n\n        self.up_conv1 = UpSample(1024, encoder_channels[-2], encoder_channels[-2])\n        self.up_conv2 = UpSample(encoder_channels[-2], encoder_channels[-3], encoder_channels[-3])\n        self.up_conv3 = UpSample(encoder_channels[-3], encoder_channels[-4], encoder_channels[-4])\n        self.up_conv4 = UpSample(encoder_channels[-4], encoder_channels[-5], encoder_channels[-5])\n\n        self.final_upsample = nn.ConvTranspose2d(encoder_channels[-5], encoder_channels[-5], kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(encoder_channels[-5], num_classes, kernel_size=1)\n        self.aux_conv1 = nn.Conv2d(encoder_channels[-2], num_classes, kernel_size=1)\n        self.aux_conv2 = nn.Conv2d(encoder_channels[-3], num_classes, kernel_size=1)\n\n    def forward(self, x):\n        features = self.encoder(x)\n        b = self.bottleneck(features[-1])\n        up1 = self.up_conv1(b, features[-2])\n        up2 = self.up_conv2(up1, features[-3])\n        up3 = self.up_conv3(up2, features[-4])\n        up4 = self.up_conv4(up3, features[-5])\n        up4 = self.final_upsample(up4)\n        final = self.final_conv(up4)\n\n        if self.training:\n            aux1 = self.aux_conv1(up1)\n            aux2 = self.aux_conv2(up2)\n            aux1 = nn.functional.interpolate(aux1, size=x.shape[2:], mode='bilinear', align_corners=False)\n            aux2 = nn.functional.interpolate(aux2, size=x.shape[2:], mode='bilinear', align_corners=False)\n            return final, aux1, aux2\n        return final\n\n# Example usage:\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CustomUNetWithEfficientNetB3(num_classes=1, pretrained=True).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T07:38:38.899419Z","iopub.execute_input":"2025-03-11T07:38:38.899820Z","iopub.status.idle":"2025-03-11T07:38:41.156309Z","shell.execute_reply.started":"2025-03-11T07:38:38.899788Z","shell.execute_reply":"2025-03-11T07:38:41.155574Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total Parameters: {total_params}\")\n\n# Display a detailed summary using torchinfo (optional)\nfrom torchinfo import summary\n\nin_channels=3\nsummary(model, input_size=(1, in_channels, 256, 256))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Define Dice + BCE Loss for multiple outputs\nclass DiceBCELoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(DiceBCELoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss()  # BCE with logits\n        self.smooth = smooth\n\n    def dice_loss(self, outputs, targets):\n        outputs = torch.sigmoid(outputs)  # Convert logits to probabilities\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        intersection = (outputs * targets).sum()\n        return 1 - (2. * intersection + self.smooth) / (outputs.sum() + targets.sum() + self.smooth)\n\n    def forward(self, outputs, targets):\n        # If outputs is a tuple (from deep supervision), handle each output separately\n        if isinstance(outputs, tuple):\n            final, aux1, aux2 = outputs  # Unpack the tuple\n            # Compute loss for each output\n            bce_final = self.bce(final, targets)\n            dice_final = self.dice_loss(final, targets)\n            bce_aux1 = self.bce(aux1, targets)\n            dice_aux1 = self.dice_loss(aux1, targets)\n            bce_aux2 = self.bce(aux2, targets)\n            dice_aux2 = self.dice_loss(aux2, targets)\n            # Combine losses with weights\n            total_loss = (bce_final + dice_final) + 0.2 * (bce_aux1 + dice_aux1) + 0.1 * (bce_aux2 + dice_aux2)\n            return total_loss\n        else:\n            # For inference mode (single output)\n            bce_loss = self.bce(outputs, targets)\n            dice_loss = self.dice_loss(outputs, targets)\n            return bce_loss + dice_loss\n\n# Assuming model is already defined (CustomUNetWithEfficientNetB3)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)  # Your model instance\n\n# Initialize the loss function and optimizer\ncriterion = DiceBCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n\nnum_epochs = 12\n\n# Training Loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for images, masks in tqdm(train_loader):\n        images = images.to(device)\n        masks = masks.to(device).float()  # Ensure masks are float for BCE\n\n        optimizer.zero_grad()\n        outputs = model(images)  # Returns tuple (final, aux1, aux2) in training mode\n\n        # Reshape masks to match outputs (B, 1, H, W)\n        masks = masks.unsqueeze(1)  # Add channel dimension if not already present\n\n        # Compute the combined Dice + BCE loss\n        loss = criterion(outputs, masks)\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n\n    # Step the scheduler at the end of each epoch\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T07:38:42.383319Z","iopub.execute_input":"2025-03-11T07:38:42.383703Z","iopub.status.idle":"2025-03-11T08:00:42.731948Z","shell.execute_reply.started":"2025-03-11T07:38:42.383673Z","shell.execute_reply":"2025-03-11T08:00:42.730846Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 251/251 [01:51<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/12], Loss: 0.6773\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:50<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/12], Loss: 0.2696\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/12], Loss: 0.2280\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:50<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/12], Loss: 0.2076\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/12], Loss: 0.1872\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/12], Loss: 0.1820\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/12], Loss: 0.1725\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/12], Loss: 0.1699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/12], Loss: 0.1603\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/12], Loss: 0.1592\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:49<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/12], Loss: 0.1567\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 251/251 [01:50<00:00,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch [12/12], Loss: 0.1538\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import jaccard_score, f1_score\n\ndef evaluate_model(model, data_loader, device):\n    model.eval()  # Set the model to evaluation mode\n    total_iou = 0\n    total_dice = 0\n    total_precision = 0\n    total_recall = 0\n    total_f1 = 0\n    num_batches = len(data_loader)\n\n    with torch.no_grad():\n        for images, masks in data_loader:\n            images = images.to(device)\n            masks = masks.to(device).float()\n\n            outputs = model(images)\n            predictions = torch.sigmoid(outputs) > 0.5  # Apply sigmoid and threshold\n            \n            # Convert predictions and masks to numpy arrays for metrics calculation\n            predictions_np = predictions.cpu().numpy().astype(int).flatten()\n            masks_np = masks.cpu().numpy().astype(int).flatten()\n\n            # Calculate metrics\n            iou = jaccard_score(masks_np, predictions_np, average='binary', zero_division=1)\n            total_iou += iou\n\n            # Calculate Dice Coefficient\n            intersection = np.sum(predictions_np * masks_np)\n            dice = (2. * intersection) / (np.sum(predictions_np) + np.sum(masks_np) + 1e-6)\n            total_dice += dice\n\n            # Calculate Precision, Recall, and F1 Score\n            tp = np.sum(predictions_np * masks_np)\n            fp = np.sum(predictions_np * (1 - masks_np))\n            fn = np.sum((1 - predictions_np) * masks_np)\n            \n            precision = tp / (tp + fp + 1e-6)\n            recall = tp / (tp + fn + 1e-6)\n            f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n            \n            total_precision += precision\n            total_recall += recall\n            total_f1 += f1\n\n    avg_iou = total_iou / num_batches\n    avg_dice = total_dice / num_batches\n    avg_precision = total_precision / num_batches\n    avg_recall = total_recall / num_batches\n    avg_f1 = total_f1 / num_batches\n\n    print(f'Average IoU: {avg_iou:.4f}')\n    print(f'Average Dice Coefficient: {avg_dice:.4f}')\n    print(f'Average Precision: {avg_precision:.4f}')\n    print(f'Average Recall: {avg_recall:.4f}')\n    print(f'Average F1 Score: {avg_f1:.4f}')\n\n# Evaluate the model on the test dataset\nevaluate_model(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:00:42.733256Z","iopub.execute_input":"2025-03-11T08:00:42.733619Z","iopub.status.idle":"2025-03-11T08:02:30.182049Z","shell.execute_reply.started":"2025-03-11T08:00:42.733592Z","shell.execute_reply":"2025-03-11T08:02:30.180959Z"}},"outputs":[{"name":"stdout","text":"Average IoU: 0.8984\nAverage Dice Coefficient: 0.9464\nAverage Precision: 0.9505\nAverage Recall: 0.9426\nAverage F1 Score: 0.9464\n","output_type":"stream"}],"execution_count":6}]}